
[参考一：通俗理解贝叶斯算法](https://blog.csdn.net/AMDS123/article/details/70173402)
[参考二：在Python中实现贝叶斯算法](https://blog.csdn.net/moxigandashu/article/details/71480251?locationNum=16&fps=1)

```python
# 训练数据，postingList是含有关键字的6个集合，分别对应classVec是否为侮辱性言论
def loadDataSet():
    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],
                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],
                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],
                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],
                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],
                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]
    classVec=[0,1,0,1,0,1] #1表示侮辱性言论，0表示正常言论
    return postingList,classVec
postingList,classVec=loadDataSet()
```


```python
postingList,classVec
```




    ([['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],
      ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],
      ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],
      ['stop', 'posting', 'stupid', 'worthless', 'garbage'],
      ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],
      ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']],
     [0, 1, 0, 1, 0, 1])




```python
# 第一步：我们便是提取训练数据中所有的单词，
# set的目的就是出重，使得每个单词对应一个位置，可以看做一个“字典”

def createVocalList(vocaArr):
    vocaSet=set([])
    for voca in vocaArr:
        vocaSet=vocaSet|set(voca)
    return list(vocaSet)
vocaList=createVocalList(postingList)
vocaList
```




    ['licks',
     'not',
     'how',
     'park',
     'posting',
     'garbage',
     'dog',
     'to',
     'take',
     'love',
     'is',
     'ate',
     'buying',
     'flea',
     'problems',
     'help',
     'worthless',
     'my',
     'quit',
     'dalmation',
     'him',
     'please',
     'so',
     'maybe',
     'has',
     'mr',
     'food',
     'stupid',
     'cute',
     'I',
     'steak',
     'stop']




```python
# 第二步：对（训练/测试）数据建立向量，长度与“字典”相同，
# 我们测试数据中的值如果在“字典”中找到，
# 则会在新建立的向量中的相同位置标记为1
from numpy import *
def setOfVaca2Vetor(vocaList,inputArr):
    returnList=zeros(len(vocaList))
    for word in inputArr:
        if word in vocaList:
            returnList[vocaList.index(word)]=1
        else:
            return "This word "+word+" is not exist in vocaList!"
    return returnList
vector = setOfVaca2Vetor(vocaList,postingList[0])

```


```python
vector
```




    array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
           1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])



$$p(类别|特征)=\frac{p(特征|类别)*p(类别)}{p(特征)}$$
* 特征是是一个向量，里面包含个特征值[特征1，特征2，特征3，...,特征n]
* `p(类别)`指的是是否为侮辱性言论，训练数据一共有6条，正常和侮辱言论各占1/2
* `p(特征)`指的是为每条言论中的关键字 训练数据中的`stupid`，`please`等都是，
* 贝叶斯之所以要朴素，是因为每个属性都保持独立，这在代码中便有所体现
* 因为特征出现的概率是固定的$p[特征1]*p[特征2]*...*p[特征n]$，即这个公式的分母是固定的，因此在分类时，只需要看分子
* $p(特征|类别)$可以看场是$p(特征1|类别)$*$p(特征2|类别)$...$p(特征n|类别)$
    因此便有$p(特征K|类别)=\frac{特征K在所有类别中出现的次数}{所有类别的特征总数}$
* 重点一定要看懂上面这一条




```python
# 第三部：重头戏！！！
# 朴素贝叶斯算法的实现

def bayes(trainDocument,classifyArr):
    # p(类别)    
    classifyResult=sum(classifyArr)/len(classifyArr)
    #训练集的条数         
    documentLen=len(trainDocument)
    #每条训练集的特征向量的长度     
    vectorLen=len(trainDocument[0])
      
    p0Num=zeros(vectorLen)
    p1Num=zeros(vectorLen)
    p0Demon=0
    p1Demon=0
    #遍历每个文档     
    for i in range(0,documentLen):
        #正常言论，         
        if(classifyArr[i]==0):
            # 统计所有正常言论中每个特征值 结果仍然为向量
            p0Num+=trainDocument[i]
            #统计所有的特征词语总数，结果为值             
            p0Demon+=sum(trainDocument[i])
        #正常言论，侮辱性言论    
        if(classifyArr[i]==1):
            # 统计所有侮辱言论中每个特征值 对应的特征值相加 结果仍然为向量
            p1Num+=trainDocument[i]
            #统计所有的特征词语总数，结果为值 
            p1Demon+=sum(trainDocument[i])
    
    p0_probability=p0Num/p0Demon
    p1_probability=p1Num/p1Demon
    return p0_probability,p1_probability,classifyResult
```


```python
# 将测试数据全部向量化
train2dArr=[]
for trainVoca in postingList:
    train2dArr.append(setOfVaca2Vetor(vocaList,trainVoca))
train2dArr
```




    [array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
            1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]),
     array([0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.]),
     array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
            1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.]),
     array([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.]),
     array([1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
            1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1.]),
     array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
            0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.])]




```python
# 调用贝叶斯算法，统计每个特征在各自的类型中出现的频率
p0_probability,p1_probability,classifyResult=bayes(train2dArr,classVec)
p0_probability,p1_probability,classifyResult
```




    (array([0.04166667, 0.        , 0.04166667, 0.        , 0.        ,
            0.        , 0.04166667, 0.04166667, 0.        , 0.04166667,
            0.04166667, 0.04166667, 0.        , 0.04166667, 0.04166667,
            0.04166667, 0.        , 0.125     , 0.        , 0.04166667,
            0.08333333, 0.04166667, 0.04166667, 0.        , 0.04166667,
            0.04166667, 0.        , 0.        , 0.04166667, 0.04166667,
            0.04166667, 0.04166667]),
     array([0.        , 0.05263158, 0.        , 0.05263158, 0.05263158,
            0.05263158, 0.10526316, 0.05263158, 0.05263158, 0.        ,
            0.        , 0.        , 0.05263158, 0.        , 0.        ,
            0.        , 0.10526316, 0.        , 0.05263158, 0.        ,
            0.05263158, 0.        , 0.        , 0.05263158, 0.        ,
            0.        , 0.05263158, 0.15789474, 0.        , 0.        ,
            0.        , 0.05263158]),
     0.5)



我们看到上面的每个特征频率都是小数不直观，因此对结果取对数
并且把p0Num 和p1Num特征值都改为1，p0Demon都改为2，至于为什么你可以理解成
$p(特征1|类别)$*$p(特征2|类别)$...$p(特征n|类别)$可能因为某个概率为0而全为0
。。。额 其实这个观点直观很好理解，但是本人在代码中理解起来却觉得这么回事，
但是也没必要太过深究



```python
# 我们看到上面的每个特征频率都是小数不直观，因此对结果取对数
# 并且把p0Num 和p1Num特征值都改为1，p0Demon都改为2，至于为什么你可以理解成
def bayesModify(trainDocument,classifyArr):
    classifyResult=sum(classifyArr)/len(classifyArr)
    documentLen=len(trainDocument)
    voctorLen=len(trainDocument[0])
    
    p0Num=ones(voctorLen)
    p1Num=ones(voctorLen)
    p0Demon=2
    p1Demon=2
    for i in range(0,documentLen):
        if(classifyArr[i]==0):
            p0Num+=trainDocument[i]
            p0Demon+=sum(trainDocument[i])
        if(classifyArr[i]==1):
            p1Num+=trainDocument[i]
            p1Demon+=sum(trainDocument[i])
    p0_probability=log(p0Num/p0Demon)
    p1_probability=log(p1Num/p1Demon)
    return p0_probability,p1_probability,classifyResult
```


```python
# 不过怎么样，这个结果却是直观了不少
p0_probability,p1_probability,classifyResult=bayesModify(train2dArr,classVec)
p0_probability,p1_probability,classifyResult
```




    (array([-2.56494936, -3.25809654, -2.56494936, -3.25809654, -3.25809654,
            -3.25809654, -2.56494936, -2.56494936, -3.25809654, -2.56494936,
            -2.56494936, -2.56494936, -3.25809654, -2.56494936, -2.56494936,
            -2.56494936, -3.25809654, -1.87180218, -3.25809654, -2.56494936,
            -2.15948425, -2.56494936, -2.56494936, -3.25809654, -2.56494936,
            -2.56494936, -3.25809654, -3.25809654, -2.56494936, -2.56494936,
            -2.56494936, -2.56494936]),
     array([-3.04452244, -2.35137526, -3.04452244, -2.35137526, -2.35137526,
            -2.35137526, -1.94591015, -2.35137526, -2.35137526, -3.04452244,
            -3.04452244, -3.04452244, -2.35137526, -3.04452244, -3.04452244,
            -3.04452244, -1.94591015, -3.04452244, -2.35137526, -3.04452244,
            -2.35137526, -3.04452244, -3.04452244, -2.35137526, -3.04452244,
            -3.04452244, -2.35137526, -1.65822808, -3.04452244, -3.04452244,
            -3.04452244, -2.35137526]),
     0.5)



验证方法，传入数据，便可以进行判断啦！
注意：$p1=sum(vec2Classify*p1Vec)+log(pClass1)$ 的数学原理是$ln(a*b)=ln(a) +ln(b)$


```python
# 贝叶斯算法最直观的体现，两种类型相比较 实则两个分子相比较
def classifyNB(vec2Classify,p0Vec,p1Vec,pClass1):
    p1=sum(vec2Classify*p1Vec)+log(pClass1)
    p0=sum(vec2Classify*p0Vec)+log(1-pClass1)
    if p1>p0:
        return 1
    else:
        return 0
```


```python
# 测试集1
testEntry=['love','my','dalmation']
```


```python
thisDoc=setOfVaca2Vetor(vocaList,testEntry)
thisDoc
```




    array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
           1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])




```python
print(testEntry,'classified as:',classifyNB(thisDoc,p0_probability,p1_probability,classifyResult))
```

    ['love', 'my', 'dalmation'] classified as: 0
    


```python
# 测试集1
testEntry=['dog','worthless','stupid']
thisDoc=setOfVaca2Vetor(vocaList,testEntry)
thisDoc
```




    array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])




```python
print(testEntry,'classified as:'
      ,classifyNB(thisDoc,p0_probability,p1_probability,classifyResult))
```

    ['dog', 'worthless', 'stupid'] classified as: 1
    
